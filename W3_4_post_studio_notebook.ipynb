{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acc85a55",
   "metadata": {},
   "source": [
    "## W3&W4 post studio exercises (errors, model fitting)\n",
    "\n",
    "Enter your solution in the cell(s) below each exercise. Add couple of inline comments explaining your code. Don't forget to add comments in markdown cell after each exercise. Missing comments (in markdown cells and/or inline) and late submissions will incur penalties.\n",
    "\n",
    "Once done, drag&drop your python file to your ADS1002-name github account.\n",
    "\n",
    "Copy url of this file on github to appropriate folder on Moodle by 09.30am prior your next studio. \n",
    "\n",
    "Solutions will be released later in the semester.\n",
    "\n",
    "Max 10 marks - 2.5 marks per each exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6239fd43",
   "metadata": {},
   "source": [
    "***\n",
    "We will use \n",
    "\n",
    "* [who-health-data.csv](https://gitlab.erc.monash.edu.au/bads/data-challenges-resources/-/tree/main/Machine-Learning/Supervised-Methods/who-health-data.csv)\n",
    "\n",
    "* [wisconsin-cancer-data.csv](https://gitlab.erc.monash.edu.au/bads/data-challenges-resources/-/tree/main/Machine-Learning/Supervised-Methods/kaggle-wisconsin-cancer.csv)\n",
    "\n",
    "throughout the exercises. Download the datasets into the same directory as your post-studio notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d36b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "who_data_2015 = (\n",
    "    pd.read_csv(\"who-health-data.csv\") # Read in the csv data.\n",
    "    .rename(columns=lambda c: c.strip())      # Clean up column names.\n",
    "    .query(\"Year == 2015\")                    # Restrict the dataset to records from 2015.\n",
    "    # Removes two columns which contain a lot of missing data...\n",
    "    .drop(columns=[\"Alcohol\", \"Total expenditure\"])\n",
    "    # ... then drop any rows with missing values.\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "wisconsin_cancer_biopsies = (\n",
    "    pd.read_csv(\"kaggle-wisconsin-cancer.csv\")\n",
    "    # This tidies up the naming of results (M -> malignant, B -> benign)\n",
    "    .assign(diagnosis=lambda df: df['diagnosis']  \n",
    "        .map({\"M\": \"malignant\", \"B\": \"benign\"})\n",
    "        .astype('category')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e823bd",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Given the dataframe `ex1_who_with_predictions` below, compute the Mean Absolute Error for the predicted values of life expectancy. You can repeat the process previously shown, or find a function in `sklearn.metrics` to compute this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0ce618f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 3.790230769230769\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Assuming who_data_2015 is already defined as per the given code\n",
    "\n",
    "ex1_who_with_predictions = (\n",
    "    who_data_2015[[\"Schooling\", \"Life expectancy\"]]\n",
    "    .assign(Predicted=lambda df: df[\"Schooling\"] * 2.3 + 43)\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Calculate the Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(\n",
    "    ex1_who_with_predictions[\"Life expectancy\"],\n",
    "    ex1_who_with_predictions[\"Predicted\"]\n",
    ")\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380e650",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Given the classification predictions and actual results in the dataframe `ex2_biopsies_with_predictions` below, compute accuracy, precision and recall. Also find the number of false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f0d649c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7311072056239016\n",
      "Precision: 0.6311111111111111\n",
      "Recall: 0.6698113207547169\n",
      "Number of False Negatives: 70\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Assuming wisconsin_cancer_biopsies is already defined as per the given code\n",
    "\n",
    "ex2_biopsies_with_predictions = (\n",
    "    wisconsin_cancer_biopsies\n",
    "    .assign(prediction=lambda df: df['texture_mean'].lt(20)\n",
    "        .map({True: \"benign\", False: \"malignant\"})\n",
    "    )\n",
    "    [['radius_mean', 'texture_mean', 'diagnosis', 'prediction']]\n",
    ")\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(\n",
    "    ex2_biopsies_with_predictions['diagnosis'],\n",
    "    ex2_biopsies_with_predictions['prediction']\n",
    ")\n",
    "\n",
    "# Compute precision\n",
    "precision = precision_score(\n",
    "    ex2_biopsies_with_predictions['diagnosis'],\n",
    "    ex2_biopsies_with_predictions['prediction'],\n",
    "    pos_label=\"malignant\"\n",
    ")\n",
    "\n",
    "# Compute recall\n",
    "recall = recall_score(\n",
    "    ex2_biopsies_with_predictions['diagnosis'],\n",
    "    ex2_biopsies_with_predictions['prediction'],\n",
    "    pos_label=\"malignant\"\n",
    ")\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(\n",
    "    ex2_biopsies_with_predictions['diagnosis'],\n",
    "    ex2_biopsies_with_predictions['prediction'],\n",
    "    labels=[\"malignant\", \"benign\"]\n",
    ")\n",
    "\n",
    "# Number of false negatives is in the first row, second column of the confusion matrix\n",
    "false_negatives = conf_matrix[0, 1]\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Number of False Negatives:\", false_negatives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2442f1dc",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Consider three different predictors for the cancer biopsy screening dataset:\n",
    "\n",
    "* Predictor A has an accuracy of 0.95, and recall of 0.99\n",
    "* Predictor B has an accuracy of 0.99, and recall of 0.95\n",
    "* Predictor C has an accuracy of 0.5, and a recall of 1.0\n",
    "\n",
    "The test required to collect data from a new patient (on which the predictor will give a predicted diagnosis) is minimally invasive. If the predictor predicts a positive (malignant) diagnosis, the patient will be referred for further screening which can be expensive.\n",
    "\n",
    "Considering the context, which predictive model (A, B, or C) would likely be preferred for this task? Write your answer in a markdown cell below, and give a brief explanation of your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a5c60",
   "metadata": {},
   "source": [
    "### Preferred Predictive Model: Predictor B\n",
    "\n",
    "#### Reasoning:\n",
    "In the context of cancer biopsy screening, the goal is to correctly identify as many true positive (malignant) cases as possible while minimizing the number of false positives.\n",
    "\n",
    "- **Predictor A** has a very high recall (0.99), meaning it correctly identifies almost all malignant cases. However, its slightly lower accuracy (0.95) suggests that there might be more false positives\n",
    "  \n",
    "- **Predictor B** offers a balance with a very high accuracy (0.99) and a reasonable recall (0.95). This model is likely to identify most malignant cases while also reducing the number of false positives, minimizing unnecessary costs.\n",
    "\n",
    "- **Predictor C** has perfect recall (1.0) but very low accuracy (0.5), indicating that it might predict almost every case as malignant, leading to a large number of false positives. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54311d40",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Choose one different input/feature variable (other than Schooling) and fit a linear regression model to predict Life Expectancy using sklearn. Can you achieve a better error rate than what we found in pre-studio notebook? (RMSE and MAE for Schooling were 4.71 and 3.69, respectively.) Suggest a method to narrow down your choices of variables to use in order to arrive at a good model. \n",
    "\n",
    "Hint 1: Correlation.\n",
    "\n",
    "Hint 2: You can use the functions written in the pre-studio notebook, e.g. prediction_root_mean_squared_error(gradient, intercept), to calculate the model error once you choose your model parameters (features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28a34f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlations with Life Expectancy:\n",
      " Year                                    NaN\n",
      "Adult Mortality                   -0.731215\n",
      "infant deaths                     -0.209304\n",
      "percentage expenditure             0.064494\n",
      "Hepatitis B                        0.372109\n",
      "Measles                           -0.049305\n",
      "BMI                                0.544987\n",
      "under-five deaths                 -0.241013\n",
      "Polio                              0.493438\n",
      "Diphtheria                         0.466223\n",
      "HIV/AIDS                          -0.620511\n",
      "GDP                                0.487018\n",
      "Population                        -0.027594\n",
      "thinness  1-19 years              -0.459153\n",
      "thinness 5-9 years                -0.454897\n",
      "Income composition of resources    0.898059\n",
      "Schooling                          0.806074\n",
      "Name: Life expectancy, dtype: float64\n",
      "Best feature for prediction (highest correlation): Income composition of resources\n",
      "RMSE: 3.504293642534378\n",
      "MAE: 2.7371964876087294\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the data (assuming who_data_2015 is already defined)\n",
    "\n",
    "# Exclude non-numeric columns before calculating correlations\n",
    "numeric_data = who_data_2015.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate correlations with Life Expectancy\n",
    "correlations = numeric_data.corr()['Life expectancy'].drop('Life expectancy')\n",
    "print(\"Correlations with Life Expectancy:\\n\", correlations)\n",
    "\n",
    "# Choose the feature with the highest correlation (besides Schooling)\n",
    "# Let's assume 'GDP' has the highest correlation for this example\n",
    "best_feature = correlations.abs().idxmax()\n",
    "print(\"Best feature for prediction (highest correlation):\", best_feature)\n",
    "\n",
    "# Step 2: Fit the Linear Regression Model using the best feature\n",
    "X = who_data_2015[[best_feature]].dropna()  # Feature matrix\n",
    "y = who_data_2015['Life expectancy'].dropna()  # Target variable\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict Life Expectancy\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Step 3: Calculate RMSE and MAE\n",
    "rmse = np.sqrt(mean_squared_error(y, predictions))\n",
    "mae = mean_absolute_error(y, predictions)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e3861",
   "metadata": {},
   "source": [
    "Comparison with the Schooling Model:\n",
    "RMSE and MAE for Schooling:\n",
    "RMSE: 4.71\n",
    "MAE: 3.69\n",
    "RMSE and MAE for Income composition of resources:\n",
    "RMSE: 3.50\n",
    "MAE: 2.74"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff23a48",
   "metadata": {},
   "source": [
    "Using \"Income composition of resources\" as the predictor outperforms the model that used \"Schooling\" in terms of both RMSE and MAE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6403204c",
   "metadata": {},
   "source": [
    "Narrowing Down Variables plans:\n",
    "\n",
    "Correlation Analysis: Start by calculating the correlation between each feature and the target variable (Life Expectancy). High absolute correlation indicates a stronger linear relationship.\n",
    "\n",
    "Model Evaluation: Fit models using the selected features and evaluate their performance using error metrics such as RMSE and MAE. Compare different models to choose the best-performing one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d047d49b",
   "metadata": {},
   "source": [
    "## Extra exercises\n",
    "\n",
    "The following exercises with (*) will not be assessed. Use these to check your understanding of topics covered in the past 2 weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7caa6",
   "metadata": {},
   "source": [
    "### Exercise 5*\n",
    "\n",
    "The function `model_correct_predictions` below returns the number of correct predictions made by a predictive model for the cancer biopsy dataset, for a given parameter value. This parameter value simply controls the threshold value for radius above which a sample is predicted as malignant.\n",
    "\n",
    "Try different values of the parameter in this model within the range [0, 30]. Record and plot the resulting accuracy values against the parameter value (similar to the regression cost function example above).\n",
    "\n",
    "What value of the parameter provides the best error rate? Explain how can you be confident you have found the best result here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cd0a26e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_correct_predictions(radius_split_parameter):\n",
    "    \"\"\" Return the number of correct predictions made by the model\n",
    "    for the given parameter value. \"\"\"\n",
    "    data = wisconsin_cancer_biopsies.assign(\n",
    "        predicted=lambda df: df['radius_mean'].lt(radius_split_parameter)\n",
    "            .map({True: \"benign\", False: \"malignant\"})\n",
    "    )\n",
    "    return (data['diagnosis'] == data['predicted']).sum()\n",
    "\n",
    "model_correct_predictions(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5418d4af",
   "metadata": {},
   "source": [
    "### Exercise 6*\n",
    "\n",
    "In examples in pre-studio notebook (W4) we have used root mean squared error (the standard cost function for linear regression) to fit the model parameters. Try re-running the `scipy.optimise` method using mean absolute error. Are the resulting model parameters the same as above? Give some brief reasoning why there might be a difference here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62dd55f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: you only need to make one small change in the prediction_error function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc09b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_root_mean_squared_error(gradient, intercept):\n",
    "    \"\"\" Return the prediction error associated with the value of the parameters.\n",
    "    This time around, let's use sklearn.metrics. \"\"\"\n",
    "    predictions = who_data_2015[\"Schooling\"] * gradient + intercept\n",
    "    actual = who_data_2015[\"Life expectancy\"]\n",
    "    # Note that `squared=False` gives us RMSE. Then we're in the same units as MAE.\n",
    "    return mean_squared_error(y_true=actual, y_pred=predictions, squared=False)\n",
    "\n",
    "def prediction_mean_absolute_error(gradient, intercept):\n",
    "    \"\"\" Return the prediction error associated with the value of the parameters.\n",
    "    This time around, let's use sklearn.metrics. \"\"\"\n",
    "    predictions = who_data_2015[\"Schooling\"] * gradient + intercept\n",
    "    actual = who_data_2015[\"Life expectancy\"]\n",
    "    return mean_absolute_error(y_true=actual, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd1b2a",
   "metadata": {},
   "source": [
    "### Exercise 7*\n",
    "\n",
    "We can see above that different methods for determining model parameters arrive at the same result, but what happens if we change the dataset slightly. Experiment by taking several (at least 10) different samples of the data, fitting a linear model for each one, and plotting a histogram of the different gradient and intercept coefficients you find. Is there a significant amount of variation in the parameter values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e345346",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = who_data_2015.sample(30)  # selects a small sample of 30 random rows from the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
